
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from datasets import Dataset
import sys
import wandb
from transformers import TrainerCallback

# Add the project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.load_openr1 import load_openr1_subset
from utils.logging_utils import make_run_dir

# Custom callback to save weight deltas
class WeightDeltaCallback(TrainerCallback):
    def __init__(self, run_dir):
        self.run_dir = run_dir
        self.previous_state_dict = None

    def on_step_begin(self, args, state, control, **kwargs):
        model = kwargs["model"]
        if self.previous_state_dict is None:
            self.previous_state_dict = {name: p.clone().detach() for name, p in model.named_parameters()}

    def on_step_end(self, args, state, control, **kwargs):
        model = kwargs["model"]
        current_state_dict = {name: p.clone().detach() for name, p in model.named_parameters()}
        
        weight_deltas = {name: current_state_dict[name] - self.previous_state_dict[name] for name in current_state_dict}

        step_dir = os.path.join(self.run_dir, f"step_{int(state.global_step)}")
        os.makedirs(step_dir, exist_ok=True)
        for name, delta in weight_deltas.items():
            if torch.all(delta == 0):
                continue
            safe_name = name.replace(".", "_")
            torch.save(delta, os.path.join(step_dir, f"{safe_name}.pt"))
        
        self.previous_state_dict = current_state_dict

def prepare_dpo_dataset(original_dataset, tokenizer, model):
    """
    Prepare a synthetic DPO dataset from a standard dataset.
    'chosen' is the ground truth, 'rejected' is generated by the base model.
    """
    dpo_data = []
    for item in original_dataset:
        prompt = item['prompt']
        chosen = item['label']

        # Generate a 'rejected' response from the base model
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
        generated_ids = model.generate(input_ids, max_length=50)
        rejected = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        dpo_data.append({
            "prompt": prompt,
            "chosen": chosen,
            "rejected": rejected
        })
    return Dataset.from_list(dpo_data)


def train(
    model_name="google/gemma-3-270m-it",
    n_steps=5,
    batch_size=1, 
    learning_rate=5e-5,
    subset_size=10, # Reduced for faster synthetic data generation
    run_name="gemma_dpo_training"
):
    """
    Train a Gemma model using DPOTrainer and log with wandb.
    """
    # Initialize wandb
    wandb.init(project="rl-casino", name=run_name, config={
        "model_name": model_name,
        "n_steps": n_steps,
        "batch_size": batch_size,
        "learning_rate": learning_rate,
        "subset_size": subset_size
    })

    # Create a directory for this run
    run_dir = make_run_dir(base_dir="results", run_name=run_name)
    print(f"Run directory: {run_dir}")

    # Load the dataset and tokenizer
    original_dataset, tokenizer = load_openr1_subset(
        tokenizer_name=model_name,
        subset_size=subset_size
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load the model
    model = AutoModelForCausalLM.from_pretrained(model_name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Prepare the DPO dataset
    print("Preparing DPO dataset...")
    dpo_dataset = prepare_dpo_dataset(original_dataset, tokenizer, model)
    print("DPO dataset prepared.")

    # Set up TrainingArguments
    training_args = TrainingArguments(
        output_dir=os.path.join(run_dir, "checkpoints"),
        per_device_train_batch_size=batch_size,
        learning_rate=learning_rate,
        max_steps=n_steps,
        logging_steps=1,
        report_to="wandb",
        remove_unused_columns=False,
    )

    # Set up the DPOTrainer
    dpo_trainer = DPOTrainer(
        model,
        ref_model=None, # DPOTrainer will create a reference model if None
        args=training_args,
        train_dataset=dpo_dataset,
        tokenizer=tokenizer,
        callbacks=[WeightDeltaCallback(run_dir=run_dir)]
    )

    # Train the model
    print("Starting DPO training...")
    dpo_trainer.train()
    print("Training finished.")
    wandb.finish()

if __name__ == "__main__":
    train()
