import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from datasets import Dataset
import sys
import wandb
from transformers import TrainerCallback

# Add the project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.load_openr1 import load_openr1_subset
from utils.logging_utils import make_run_dir

WANDB_PROJECT = "rl-casino"
MODEL_NAME = "google/gemma-3-270m-it"
DATASET_NAME = "Light-R1"
SUBSET_SIZE = 10


# Custom callback to save weight deltas
class WeightDeltaCallback(TrainerCallback):
    def __init__(self, run_dir):
        self.run_dir = run_dir
        self.previous_state_dict = None

    def on_train_begin(self, args, state, control, **kwargs):
        # Capture initial weights
        model = kwargs["model"]
        self.previous_state_dict = {name: p.clone().detach().cpu() for name, p in model.named_parameters()}

    def on_step_end(self, args, state, control, **kwargs):
        model = kwargs["model"]
        current_state_dict = {name: p.clone().detach().cpu() for name, p in model.named_parameters()}
        
        weight_deltas = {name: current_state_dict[name] - self.previous_state_dict[name] for name in current_state_dict}

        step_dir = os.path.join(self.run_dir, f"step_{int(state.global_step)}")
        os.makedirs(step_dir, exist_ok=True)
        
        # Save deltas to disk
        for name, delta in weight_deltas.items():
            if torch.all(delta == 0):
                continue
            safe_name = name.replace(".", "_")
            torch.save(delta, os.path.join(step_dir, f"{safe_name}.pt"))
        
        # Update previous state
        self.previous_state_dict = current_state_dict


def prepare_dpo_dataset(original_dataset, tokenizer, model):
    """
    Prepare a synthetic DPO dataset from a standard dataset.
    'chosen' is the ground truth, 'rejected' is generated by the base model.
    """
    dpo_data = []
    for item in original_dataset:
        prompt = item['prompt']
        chosen = item['label']

        # Generate a 'rejected' response from the base model
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        input_ids = inputs.input_ids.to(model.device)
        attention_mask = inputs.attention_mask.to(model.device)
        
        generated_ids = model.generate(
            input_ids, 
            attention_mask=attention_mask,
            max_new_tokens=20,  # Generate 20 new tokens
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,  # Add some randomness for variety
            temperature=0.8
        )
        rejected = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        dpo_data.append({
            "prompt": prompt,
            "chosen": chosen,
            "rejected": rejected
        })
    return Dataset.from_list(dpo_data)


def train(
    model_name="google/gemma-3-270m-it",
    n_steps=5,
    batch_size=1, 
    learning_rate=5e-5,
    subset_size=10,
    run_name="gemma_dpo_training"
):
    """
    Train a Gemma model using DPOTrainer and log with wandb.
    """
    # Create a directory for this run
    run_dir = make_run_dir(base_dir="results", run_name=run_name)
    print(f"Run directory: {run_dir}")

    # Load the dataset and tokenizer
    original_dataset, tokenizer = load_openr1_subset(
        tokenizer_name=model_name,
        subset_size=subset_size
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load the model (use float16 on GPU for efficiency)
    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Model loaded on {device}")

    # Prepare the DPO dataset
    print("Preparing DPO dataset (generating rejected responses)...")
    dpo_dataset = prepare_dpo_dataset(original_dataset, tokenizer, model)
    print(f"DPO dataset prepared with {len(dpo_dataset)} examples.")

    # Set up DPOConfig (not TrainingArguments!)
    dpo_config = DPOConfig(
        output_dir=os.path.join(run_dir, "checkpoints"),
        per_device_train_batch_size=batch_size,
        learning_rate=learning_rate,
        max_steps=n_steps,
        logging_steps=1,
        report_to="wandb",
        remove_unused_columns=False,
        run_name=run_name,
        gradient_accumulation_steps=1,
    )

    # Initialize wandb (only once, here)
    wandb.init(
        project=WANDB_PROJECT, 
        name=run_name, 
        config={
            "model_name": model_name,
            "dataset": DATASET_NAME,
            "n_steps": n_steps,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "subset_size": subset_size
        }
    )

    # Set up the DPOTrainer
    print("Initializing DPOTrainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,  # DPOTrainer will create a reference model if None
        args=dpo_config,
        train_dataset=dpo_dataset,
        processing_class=tokenizer,  # They call it "processing_class" instead of "tokenizer"
        callbacks=[WeightDeltaCallback(run_dir=run_dir)]
    )

    # Train the model
    print("Starting DPO training...")
    dpo_trainer.train()
    print("Training finished.")
    
    # Save final model
    final_model_path = os.path.join(run_dir, "final_model")
    model.save_pretrained(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    print(f"Final model saved to {final_model_path}")
    
    wandb.finish()


if __name__ == "__main__":
    train()